{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The PDF Text Search and Indexing System for Electronic Datasheets is a specialized tool designed to extract and process text from electronic datasheets in PDF format. This system enables users to efficiently search for specific terms or components across multiple datasheets and retrieve a list of documents that contain those terms. The project utilizes PyMuPDF for handling PDF files, Tesseract OCR for text extraction from images, and NLTK for text processing, including tokenization, stopword removal, and stemming. The extracted data is indexed and saved for quick word search functionality, making it particularly useful for managing and analyzing technical datasheets in industries such as electronics, engineering, and manufacturing"
      ],
      "metadata": {
        "id": "P0jH2JmTMDH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download the file from this link https://drive.google.com/drive/folders/1jAT7h6jPOfPdq8T_jJXKyY_3bzxDsOuM?usp=sharing"
      ],
      "metadata": {
        "id": "s5eqi8u0MG1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "attention : I used chat gpt to understand how scraping text from image & tables thene i tried to combined with what i have learned from the TPS also i used for stracturing the code"
      ],
      "metadata": {
        "id": "ezynV1YSMU0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI4AW6t3eXnX",
        "outputId": "f6c8e87a-d948-4b60-e60f-c4e020320935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Downloading PyMuPDF-1.24.13-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract, pymupdf\n",
            "Successfully installed pymupdf-1.24.13 pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "pip install pymupdf pytesseract nltk sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6hADWLIi5NH",
        "outputId": "4544ad49-a60d-4ae0-c867-c0cbf62a9577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install pytesseract\n",
        "!pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlnq6tHjjCiw",
        "outputId": "3c3b723e-561e-4bd4-82e8-6bf41eb9b2c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,609 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,459 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,696 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,353 kB]\n",
            "Fetched 19.0 MB in 2s (7,710 kB/s)\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,567 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "# Install Tesseract OCR\n",
        "!apt-get update -q\n",
        "!apt-get install -y tesseract-ocr\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for handling PDF files\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer  # For stemming words\n",
        "import pickle  # Used for saving and loading processed data\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download the necessary NLTK data files (for tokenization and stopwords)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the stemmer to reduce words to their root form (e.g., \"running\" becomes \"run\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)  # Open the PDF using PyMuPDF\n",
        "    text = ''  # Initialize an empty string to store extracted text\n",
        "\n",
        "    # Loop through all the pages in the PDF\n",
        "    for page_num in range(doc.page_count):\n",
        "        page = doc.load_page(page_num)  # Load the page\n",
        "        text += page.get_text(\"text\")  # Extract and add the text from this page\n",
        "\n",
        "        # If there are images in the PDF, use OCR to extract any text from them\n",
        "        pix = page.get_pixmap()  # Convert the page to an image\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)  # Convert to a PIL image\n",
        "        ocr_text = pytesseract.image_to_string(img)  # Use Tesseract OCR to extract text from the image\n",
        "        text += ocr_text  # Add the OCR text to the main text\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to process the text (tokenize, remove stopwords, and stem words)\n",
        "def process_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Tokenize the text and convert to lowercase\n",
        "\n",
        "    # Remove common stopwords (like \"the\", \"and\", \"is\") to focus on important words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Stem the words to get their root form (e.g., \"running\" → \"run\")\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Function to process all PDFs in a given directory and create a mapping of words to PDFs\n",
        "def process_pdfs_in_directory(directory_path):\n",
        "    word_pdf_mapping = defaultdict(list)  # Use a dictionary to map words to PDFs\n",
        "\n",
        "    # Loop through all PDF files in the directory\n",
        "    for pdf_file in os.listdir(directory_path):\n",
        "        if pdf_file.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
        "            pdf_path = os.path.join(directory_path, pdf_file)  # Get the full path of the PDF\n",
        "            print(f\"Processing {pdf_file}...\")  # Show progress\n",
        "\n",
        "            # Extract text from the PDF\n",
        "            text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "            # Process the text (tokenize, remove stopwords, and stem words)\n",
        "            stemmed_words = process_text(text)\n",
        "\n",
        "            # Update the word-to-PDF mapping\n",
        "            for word in stemmed_words:\n",
        "                word_pdf_mapping[word].append(pdf_file)\n",
        "\n",
        "    return word_pdf_mapping\n",
        "\n",
        "# Function to search for a word and find out in which PDFs it appears\n",
        "def search_word(word, word_pdf_mapping):\n",
        "    word = word.lower()  # Make the search case-insensitive\n",
        "    if word in word_pdf_mapping:\n",
        "        return word_pdf_mapping[word]  # Return the list of PDFs containing the word\n",
        "    else:\n",
        "        return f\"The word '{word}' does not appear in any PDF.\"  # Return a message if no PDFs contain the word\n",
        "\n",
        "# Function to save the word-to-PDF mapping to a file (to avoid reprocessing the PDFs)\n",
        "def save_word_pdf_mapping(word_pdf_mapping, filename='word_pdf_mapping.pkl'):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(word_pdf_mapping, file)  # Save the mapping using Pickle\n",
        "    print(f\"Word-to-PDF mapping saved to {filename}.\")  # Confirm saving\n",
        "\n",
        "# Function to load the word-to-PDF mapping from a file (if already saved)\n",
        "def load_word_pdf_mapping(filename='word_pdf_mapping.pkl'):\n",
        "    if os.path.exists(filename):  # Check if the saved file exists\n",
        "        with open(filename, 'rb') as file:\n",
        "            word_pdf_mapping = pickle.load(file)  # Load the saved mapping\n",
        "        print(f\"Word-to-PDF mapping loaded from {filename}.\")  # Confirm loading\n",
        "        return word_pdf_mapping\n",
        "    else:\n",
        "        print(f\"No saved mapping found. Please process PDFs first.\")  # If the file doesn't exist\n",
        "        return None\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = '/content/pdffiles'  # Set the path to your PDFs\n",
        "\n",
        "    # Try to load previously saved word-to-PDF mapping\n",
        "    word_pdf_mapping = load_word_pdf_mapping()\n",
        "\n",
        "    # If there's no saved mapping, process the PDFs and save the mapping\n",
        "    if word_pdf_mapping is None:\n",
        "        word_pdf_mapping = process_pdfs_in_directory(directory_path)\n",
        "        save_word_pdf_mapping(word_pdf_mapping)\n",
        "\n",
        "    # Start a loop to allow the user to search for multiple words\n",
        "    while True:\n",
        "        word_to_search = input(\"Enter a word to search: \")  # Ask the user for a word\n",
        "\n",
        "        # Search for the word in the mapping and display the result\n",
        "        result = search_word(word_to_search, word_pdf_mapping)\n",
        "        print(\"PDFs containing the word:\", result)\n",
        "\n",
        "        # Ask the user if they want to search for another word\n",
        "        while True:\n",
        "            repeat_search = input(\"Do you want to search for another word? (y/n): \").strip().lower()\n",
        "\n",
        "            # Validate user input to handle unexpected responses\n",
        "            if repeat_search == 'y':\n",
        "                break  # Continue with another search\n",
        "            elif repeat_search == 'n':\n",
        "                print(\"Thank you for using the search tool!\")  # Friendly exit message\n",
        "                exit()  # Exit the program\n",
        "            else:\n",
        "                print(\"Please enter 'y' for yes or 'n' for no.\")  # Prompt user for correct input\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU7mR1Zj_Mqj",
        "outputId": "3116f93c-4569-468a-81a1-b841c290e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word-to-PDF mapping loaded from word_pdf_mapping.pkl.\n",
            "PDFs containing the word: ['media (11).pdf', 'media (11).pdf', 'media (11).pdf', 'media (8).pdf', 'media (8).pdf', 'media (8).pdf', 'media (6).pdf', 'media (6).pdf', 'media (6).pdf', 'media (6).pdf', 'media (5).pdf', 'media (5).pdf', 'media (5).pdf', 'media (3).pdf', 'media (3).pdf', 'media (12).pdf', 'media (12).pdf', 'media (9).pdf', 'media (9).pdf', 'media (9).pdf', 'media (2).pdf', 'media (2).pdf', 'media (2).pdf']\n",
            "PDFs containing the word: ['ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (1).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'ne555 (6).pdf', 'media (11).pdf', 'media (11).pdf', 'media (11).pdf', 'media (11).pdf', 'media (8).pdf', 'media (8).pdf', 'media (8).pdf', 'media (8).pdf', 'media (6).pdf', 'media (6).pdf', 'media (6).pdf', 'media (6).pdf', 'media (6).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (4).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (3).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'ne555 (5).pdf', 'media (5).pdf', 'media (5).pdf', 'media (5).pdf', 'media (5).pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555.pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'ne555 (2).pdf', 'media (3).pdf', 'media (3).pdf', 'media (3).pdf', 'media (3).pdf', 'media (12).pdf', 'media (12).pdf', 'media (12).pdf', 'media (12).pdf', 'media (9).pdf', 'media (9).pdf', 'media (9).pdf', 'media (9).pdf', 'media (2).pdf', 'media (2).pdf', 'media (2).pdf', 'media (2).pdf']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}